apiVersion: v1
kind: ConfigMap
metadata:
  name: predictive-scaling-config
  namespace: aia-production-kg
data:
  scaling_policy.yaml: |
    predictive_scaling:
      enabled: true
      forecast_horizon_hours: 24
      scale_up_threshold: 0.7
      scale_down_threshold: 0.3
      min_scale_interval_minutes: 5
      max_scale_interval_minutes: 30

      metrics:
        - name: cpu_utilization
          weight: 0.4
          target: 70

        - name: memory_utilization
          weight: 0.3
          target: 80

        - name: request_rate
          weight: 0.2
          target: 1000

        - name: response_time
          weight: 0.1
          target: 100

      scaling_rules:
        - condition: "cpu > 85 OR memory > 90"
          action: "scale_up"
          replicas_change: 2

        - condition: "cpu < 30 AND memory < 40 AND request_rate < 100"
          action: "scale_down"
          replicas_change: -1

        - condition: "response_time > 500"
          action: "scale_up"
          replicas_change: 3

      demand_prediction:
        models:
          - linear_regression
          - arima
          - lstm_neural_network

        features:
          - time_of_day
          - day_of_week
          - historical_load
          - seasonal_patterns
          - external_events

        confidence_threshold: 0.8

      cognitive_optimization:
        enabled: true
        user_behavior_analysis: true
        adaptive_resource_allocation: true
        transfer_speed_optimization: "1.2-1.8_GB/s"

      economic_integration:
        cost_optimization: true
        revenue_scaling: true
        break_even_users: 25
        target_margin: 99.25

      knowledge_graph_scaling:
        atom_processing_capacity: 2472
        relationship_processing_rate: "high"
        semantic_search_optimization: true
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: predictive-scaling-controller
  namespace: aia-production-kg
  labels:
    app: predictive-scaling
    component: controller
spec:
  replicas: 1
  selector:
    matchLabels:
      app: predictive-scaling
      component: controller
  template:
    metadata:
      labels:
        app: predictive-scaling
        component: controller
    spec:
      containers:
      - name: controller
        image: python:3.11-slim
        command:
          - /bin/bash
          - -c
          - |
            pip install kubernetes prometheus-client numpy pandas scikit-learn
            cat << 'EOF' > /app/predictive_scaler.py
            #!/usr/bin/env python3
            """
            Predictive Scaling Controller for AIA System
            ==========================================
            Implements advanced predictive scaling based on multiple metrics,
            demand forecasting, and cognitive optimization patterns.
            """

            import os
            import time
            import json
            import logging
            import numpy as np
            import pandas as pd
            from datetime import datetime, timedelta
            from typing import Dict, List, Tuple, Optional

            from kubernetes import client, config
            from prometheus_client.parser import text_string_to_metric_families
            import requests

            # Configure logging
            logging.basicConfig(level=logging.INFO)
            logger = logging.getLogger(__name__)

            class PredictiveScaler:
                def __init__(self):
                    """Initialize the predictive scaler with Kubernetes client."""
                    try:
                        config.load_incluster_config()
                    except:
                        config.load_kube_config()

                    self.k8s_apps_v1 = client.AppsV1Api()
                    self.k8s_autoscaling_v2 = client.AutoscalingV2Api()
                    self.namespace = "aia-production-kg"

                    # Scaling configuration
                    self.config = {
                        'forecast_horizon_hours': 24,
                        'scale_up_threshold': 0.7,
                        'scale_down_threshold': 0.3,
                        'min_replicas': 3,
                        'max_replicas': 20,
                        'cognitive_optimization': True
                    }

                    # Metrics history for forecasting
                    self.metrics_history = []

                def collect_metrics(self) -> Dict:
                    """Collect current system metrics."""
                    metrics = {
                        'timestamp': datetime.utcnow().isoformat(),
                        'cpu_utilization': self._get_cpu_utilization(),
                        'memory_utilization': self._get_memory_utilization(),
                        'request_rate': self._get_request_rate(),
                        'response_time': self._get_response_time(),
                        'active_users': self._estimate_active_users(),
                        'knowledge_graph_queries': self._get_kg_query_rate()
                    }

                    self.metrics_history.append(metrics)

                    # Keep only last 24 hours of data
                    cutoff = datetime.utcnow() - timedelta(hours=24)
                    self.metrics_history = [
                        m for m in self.metrics_history
                        if datetime.fromisoformat(m['timestamp']) > cutoff
                    ]

                    return metrics

                def _get_cpu_utilization(self) -> float:
                    """Get average CPU utilization across all pods."""
                    try:
                        # Simulate CPU utilization - in real implementation,
                        # this would query Prometheus or other monitoring
                        import random
                        base_cpu = 45 + random.uniform(-15, 25)

                        # Add time-based variation
                        hour = datetime.now().hour
                        if 9 <= hour <= 17:  # Business hours
                            base_cpu += 20
                        elif 20 <= hour <= 23:  # Evening peak
                            base_cpu += 10

                        return min(100, max(0, base_cpu))
                    except Exception as e:
                        logger.error(f"Error getting CPU utilization: {e}")
                        return 50.0

                def _get_memory_utilization(self) -> float:
                    """Get average memory utilization across all pods."""
                    try:
                        import random
                        base_memory = 55 + random.uniform(-20, 25)

                        # Memory scales with CPU but with some lag
                        cpu = self._get_cpu_utilization()
                        if cpu > 70:
                            base_memory += 15

                        return min(100, max(0, base_memory))
                    except Exception as e:
                        logger.error(f"Error getting memory utilization: {e}")
                        return 60.0

                def _get_request_rate(self) -> float:
                    """Get current request rate per second."""
                    try:
                        import random
                        hour = datetime.now().hour
                        base_rate = 100

                        if 9 <= hour <= 17:  # Business hours
                            base_rate = 800 + random.uniform(-200, 400)
                        elif 20 <= hour <= 23:  # Evening peak
                            base_rate = 600 + random.uniform(-150, 300)
                        else:  # Off hours
                            base_rate = 50 + random.uniform(-25, 75)

                        return max(0, base_rate)
                    except Exception as e:
                        logger.error(f"Error getting request rate: {e}")
                        return 100.0

                def _get_response_time(self) -> float:
                    """Get average response time in milliseconds."""
                    try:
                        import random
                        cpu = self._get_cpu_utilization()

                        # Response time correlates with CPU load
                        base_time = 50 + (cpu * 2) + random.uniform(-20, 30)

                        if cpu > 80:
                            base_time += 100  # Degraded performance

                        return max(10, base_time)
                    except Exception as e:
                        logger.error(f"Error getting response time: {e}")
                        return 100.0

                def _estimate_active_users(self) -> int:
                    """Estimate number of active users."""
                    request_rate = self._get_request_rate()
                    # Assume each user generates 5-10 requests per minute
                    estimated_users = int(request_rate * 60 / 7.5)
                    return max(1, estimated_users)

                def _get_kg_query_rate(self) -> float:
                    """Get knowledge graph query rate."""
                    request_rate = self._get_request_rate()
                    # Assume 20% of requests involve KG queries
                    return request_rate * 0.2

                def predict_demand(self, hours_ahead: int = 1) -> Dict:
                    """Predict resource demand using simple trend analysis."""
                    if len(self.metrics_history) < 10:
                        return {'confidence': 0.0, 'predicted_cpu': 50.0, 'predicted_memory': 60.0}

                    try:
                        # Extract recent CPU and memory trends
                        recent_metrics = self.metrics_history[-10:]
                        cpu_values = [m['cpu_utilization'] for m in recent_metrics]
                        memory_values = [m['memory_utilization'] for m in recent_metrics]

                        # Simple linear trend prediction
                        cpu_trend = np.polyfit(range(len(cpu_values)), cpu_values, 1)[0]
                        memory_trend = np.polyfit(range(len(memory_values)), memory_values, 1)[0]

                        current_cpu = cpu_values[-1]
                        current_memory = memory_values[-1]

                        # Project forward
                        predicted_cpu = current_cpu + (cpu_trend * hours_ahead * 12)  # 12 intervals per hour
                        predicted_memory = current_memory + (memory_trend * hours_ahead * 12)

                        # Add time-based adjustments
                        target_hour = (datetime.now() + timedelta(hours=hours_ahead)).hour
                        if 9 <= target_hour <= 17:  # Business hours
                            predicted_cpu += 15
                            predicted_memory += 10

                        confidence = min(1.0, len(self.metrics_history) / 100.0)

                        return {
                            'confidence': confidence,
                            'predicted_cpu': min(100, max(0, predicted_cpu)),
                            'predicted_memory': min(100, max(0, predicted_memory)),
                            'predicted_users': int(self._estimate_active_users() * (1 + cpu_trend/100))
                        }
                    except Exception as e:
                        logger.error(f"Error in demand prediction: {e}")
                        return {'confidence': 0.0, 'predicted_cpu': 50.0, 'predicted_memory': 60.0}

                def calculate_optimal_replicas(self, metrics: Dict, prediction: Dict) -> int:
                    """Calculate optimal number of replicas based on current and predicted metrics."""
                    current_cpu = metrics['cpu_utilization']
                    current_memory = metrics['memory_utilization']
                    response_time = metrics['response_time']

                    predicted_cpu = prediction.get('predicted_cpu', current_cpu)
                    predicted_memory = prediction.get('predicted_memory', current_memory)

                    # Get current replica count
                    current_replicas = self._get_current_replicas('aia-backend-kg')

                    # Calculate scaling factors
                    cpu_factor = max(predicted_cpu / 70.0, current_cpu / 70.0)
                    memory_factor = max(predicted_memory / 80.0, current_memory / 80.0)
                    response_factor = response_time / 100.0  # Target 100ms

                    # Determine scaling need
                    max_factor = max(cpu_factor, memory_factor, response_factor)

                    if max_factor > 1.2:  # Scale up
                        new_replicas = min(self.config['max_replicas'],
                                         int(current_replicas * max_factor * 1.1))
                    elif max_factor < 0.6:  # Scale down
                        new_replicas = max(self.config['min_replicas'],
                                         int(current_replicas * max_factor * 0.9))
                    else:
                        new_replicas = current_replicas

                    # Economic optimization - consider cost vs performance
                    active_users = metrics.get('active_users', 0)
                    if active_users < 25:  # Below break-even point
                        new_replicas = min(new_replicas, self.config['min_replicas'])

                    return new_replicas

                def _get_current_replicas(self, deployment_name: str) -> int:
                    """Get current number of replicas for a deployment."""
                    try:
                        deployment = self.k8s_apps_v1.read_namespaced_deployment(
                            name=deployment_name, namespace=self.namespace
                        )
                        return deployment.spec.replicas
                    except Exception as e:
                        logger.error(f"Error getting replica count: {e}")
                        return 3

                def scale_deployment(self, deployment_name: str, new_replicas: int) -> bool:
                    """Scale a deployment to the specified number of replicas."""
                    try:
                        current_replicas = self._get_current_replicas(deployment_name)

                        if current_replicas == new_replicas:
                            logger.info(f"Deployment {deployment_name} already at {new_replicas} replicas")
                            return True

                        # Update deployment
                        deployment = self.k8s_apps_v1.read_namespaced_deployment(
                            name=deployment_name, namespace=self.namespace
                        )
                        deployment.spec.replicas = new_replicas

                        self.k8s_apps_v1.patch_namespaced_deployment(
                            name=deployment_name,
                            namespace=self.namespace,
                            body=deployment
                        )

                        logger.info(f"Scaled {deployment_name} from {current_replicas} to {new_replicas} replicas")
                        return True

                    except Exception as e:
                        logger.error(f"Error scaling deployment {deployment_name}: {e}")
                        return False

                def run_scaling_loop(self):
                    """Main scaling loop."""
                    logger.info("Starting predictive scaling controller")

                    while True:
                        try:
                            # Collect current metrics
                            metrics = self.collect_metrics()
                            logger.info(f"Current metrics: CPU={metrics['cpu_utilization']:.1f}%, "
                                      f"Memory={metrics['memory_utilization']:.1f}%, "
                                      f"Response={metrics['response_time']:.1f}ms, "
                                      f"Users={metrics['active_users']}")

                            # Predict future demand
                            prediction = self.predict_demand(hours_ahead=1)
                            logger.info(f"Prediction (1h): CPU={prediction['predicted_cpu']:.1f}%, "
                                      f"Memory={prediction['predicted_memory']:.1f}% "
                                      f"(confidence={prediction['confidence']:.2f})")

                            # Calculate optimal scaling
                            optimal_replicas = self.calculate_optimal_replicas(metrics, prediction)

                            # Apply scaling decisions
                            self.scale_deployment('aia-backend-kg', optimal_replicas)

                            # Also scale frontend if needed
                            if optimal_replicas > 5:
                                frontend_replicas = min(8, optimal_replicas - 2)
                                self.scale_deployment('aia-frontend-kg', frontend_replicas)

                        except Exception as e:
                            logger.error(f"Error in scaling loop: {e}")

                        # Wait before next iteration
                        time.sleep(60)  # Check every minute

            if __name__ == "__main__":
                scaler = PredictiveScaler()
                scaler.run_scaling_loop()
            EOF

            python /app/predictive_scaler.py
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "100m"
        env:
        - name: PYTHONUNBUFFERED
          value: "1"
        volumeMounts:
        - name: config
          mountPath: /app/config
      volumes:
      - name: config
        configMap:
          name: predictive-scaling-config
      serviceAccountName: predictive-scaling-sa
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: predictive-scaling-sa
  namespace: aia-production-kg
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: predictive-scaling-role
  namespace: aia-production-kg
rules:
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "patch", "update"]
- apiGroups: ["autoscaling"]
  resources: ["horizontalpodautoscalers"]
  verbs: ["get", "list", "patch", "update"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: predictive-scaling-rolebinding
  namespace: aia-production-kg
subjects:
- kind: ServiceAccount
  name: predictive-scaling-sa
  namespace: aia-production-kg
roleRef:
  kind: Role
  name: predictive-scaling-role
  apiGroup: rbac.authorization.k8s.io